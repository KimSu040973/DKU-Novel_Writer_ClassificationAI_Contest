{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데모"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufcGB_euCc7M"
   },
   "source": [
    "## 라이브러리 import 및 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:54:38.369575Z",
     "start_time": "2020-11-16T00:54:38.010782Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T01:25:01.132325Z",
     "start_time": "2020-11-16T01:25:01.111357Z"
    },
    "id": "TmA1Au6CCaC7"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from matplotlib import rcParams, pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:54:44.353382Z",
     "start_time": "2020-11-16T00:54:44.319500Z"
    }
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (16, 8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:54:47.561239Z",
     "start_time": "2020-11-16T00:54:47.533010Z"
    },
    "id": "0ZwTCLUOCtUu"
   },
   "outputs": [],
   "source": [
    "data_dir = Path('../data')\n",
    "feature_dir = Path('../feature')\n",
    "val_dir = Path('../val')\n",
    "tst_dir = Path('../tst')\n",
    "sub_dir = Path('../sub')\n",
    "\n",
    "dirs = [feature_dir, val_dir, tst_dir, sub_dir]\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:55:06.565694Z",
     "start_time": "2020-11-16T00:55:06.532889Z"
    }
   },
   "outputs": [],
   "source": [
    "algo_name = 'mta'\n",
    "feature_name = 'emb'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "sub_file = sub_dir / f'{model_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:55:08.246151Z",
     "start_time": "2020-11-16T00:55:07.366099Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "SvX_U2ETC7vA",
    "outputId": "25b26c45-fe51-42b4-b55d-eaddea79fd94"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "index                                                           \n",
       "0      He was almost choking. There was so much, so m...       3\n",
       "1                 “Your sister asked for it, I suppose?”       2\n",
       "2       She was engaged one day as she walked, in per...       1\n",
       "3      The captain was in the porch, keeping himself ...       4\n",
       "4      “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(trn_file, index_col=0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:55:08.631010Z",
     "start_time": "2020-11-16T00:55:08.248370Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "8MxZsr6yCshr",
    "outputId": "e8a76d96-2fde-4796-be03-7e3cdf4f9ff4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "0      “Not at all. I think she is one of the most ch...\n",
       "1      \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      As the lady had stated her intention of scream...\n",
       "3      “And then suddenly in the silence I heard a so...\n",
       "4      His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(tst_file, index_col=0)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPq0vxJVDS3R"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MultiHeadSelfAttention` and `TransformerBlock` from the [Text classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/) tutorial at the Keras website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:57:33.749738Z",
     "start_time": "2020-11-16T00:57:33.721143Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T00:59:46.899456Z",
     "start_time": "2020-11-16T00:59:46.876159Z"
    },
    "id": "DZdWzRkCDovd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879,) (19617,) (54879,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train['text'].values\n",
    "X_test = test['text'].values\n",
    "y = train['author'].values\n",
    "print(X_train.shape, X_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 최대 길이 : 2500\n",
      " 평균 길이 : 228.1155633302356\n"
     ]
    }
   ],
   "source": [
    "print(' 최대 길이 : {}'.format(max(len(l) for l in X_train)))\n",
    "print(' 평균 길이 : {}'.format(sum(map(len, X_train))/len(X_train))) \n",
    "#EDA분포에서 어느정도 손실 감안할지 확인하여 최대 길이 500으로 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T01:15:36.614629Z",
     "start_time": "2020-11-16T01:15:36.582589Z"
    },
    "id": "vf_TGrbKCaDK"
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions.keys()))\n",
    "def expand_contractions(s, contractions = contractions):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(expand_contractions)\n",
    "test['text'] = test['text'].apply(expand_contractions)\n",
    "\n",
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller \n",
    "\n",
    "#create function to spell check strings\n",
    "def spell_check(x):\n",
    "    spell = Speller(lang='en')\n",
    "    return \" \".join([spell(i) for i in x.split()])\n",
    "total['text'] = total['text'].apply(spell_check) ##계산적으로 무거움..시도 X\n",
    "\n",
    "train['text'] = train['text'].apply(spell_check)\n",
    "test['text'] = test['text'].apply(spell_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks1FhSNUHrke"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:49:49.085442Z",
     "start_time": "2020-11-16T03:49:49.065196Z"
    },
    "id": "um6uZK5EDzAm"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "maxlen = 500\n",
    "#EDA분포에서 어느정도 손실 감안할지 확인하여 최대 길이 500으로 정함\n",
    "embed_dim = 64\n",
    "num_heads = 4  # Number of attention heads\n",
    "padding_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:49:52.225588Z",
     "start_time": "2020-11-16T03:49:51.228674Z"
    },
    "id": "UvR9_VnTD8L9"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:49:54.936054Z",
     "start_time": "2020-11-16T03:49:53.798091Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:49:55.620258Z",
     "start_time": "2020-11-16T03:49:55.268576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 500) (19617, 500)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "trn = keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=maxlen)\n",
    "tst = keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=maxlen)\n",
    "print(trn.shape, tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:49:56.861613Z",
     "start_time": "2020-11-16T03:49:56.843912Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:55:47.896791Z",
     "start_time": "2020-11-16T03:55:47.873994Z"
    },
    "id": "U2CxfUPZEOu0"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "    inputs = layers.Input(shape=(maxlen,))\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(x)\n",
    "    x = transformer_block(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(20, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    outputs = layers.Dense(n_class, activation=\"softmax\")(x)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T04:00:43.850600Z",
     "start_time": "2020-11-16T03:55:48.234438Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "Epoch 1/12\n",
      "686/686 [==============================] - 1559s 2s/step - loss: 1.2705 - val_loss: 0.9351\n",
      "Epoch 2/12\n",
      "686/686 [==============================] - 1532s 2s/step - loss: 0.7439 - val_loss: 0.7649\n",
      "Epoch 3/12\n",
      "686/686 [==============================] - 1528s 2s/step - loss: 0.4974 - val_loss: 0.8015\n",
      "Epoch 4/12\n",
      "686/686 [==============================] - 1528s 2s/step - loss: 0.3777 - val_loss: 0.9175\n",
      "Epoch 5/12\n",
      "686/686 [==============================] - 1531s 2s/step - loss: 0.3079 - val_loss: 1.0021\n",
      "Epoch 6/12\n",
      "686/686 [==============================] - 1529s 2s/step - loss: 0.2604 - val_loss: 1.1193\n",
      "Epoch 7/12\n",
      "686/686 [==============================] - 1529s 2s/step - loss: 0.2271 - val_loss: 1.2194\n",
      "Epoch 8/12\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2025Restoring model weights from the end of the best epoch.\n",
      "686/686 [==============================] - 1531s 2s/step - loss: 0.2025 - val_loss: 1.2579\n",
      "Epoch 00008: early stopping\n",
      "training model for CV #2\n",
      "Epoch 1/12\n",
      "686/686 [==============================] - 1531s 2s/step - loss: 1.4267 - val_loss: 1.0971\n",
      "Epoch 2/12\n",
      "686/686 [==============================] - 1529s 2s/step - loss: 0.8794 - val_loss: 0.7815\n",
      "Epoch 3/12\n",
      "686/686 [==============================] - 1552s 2s/step - loss: 0.5756 - val_loss: 0.7949\n",
      "Epoch 4/12\n",
      "686/686 [==============================] - 1526s 2s/step - loss: 0.4198 - val_loss: 0.8480\n",
      "Epoch 5/12\n",
      "686/686 [==============================] - 1529s 2s/step - loss: 0.3392 - val_loss: 0.9718\n",
      "Epoch 6/12\n",
      "686/686 [==============================] - 1529s 2s/step - loss: 0.2778 - val_loss: 1.1325\n",
      "Epoch 7/12\n",
      "686/686 [==============================] - 1523s 2s/step - loss: 0.2415 - val_loss: 1.2037\n",
      "Epoch 8/12\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2126Restoring model weights from the end of the best epoch.\n",
      "686/686 [==============================] - 1521s 2s/step - loss: 0.2126 - val_loss: 1.2234\n",
      "Epoch 00008: early stopping\n",
      "training model for CV #3\n",
      "Epoch 1/12\n",
      "686/686 [==============================] - 1523s 2s/step - loss: 1.4321 - val_loss: 1.0435\n",
      "Epoch 2/12\n",
      "686/686 [==============================] - 1522s 2s/step - loss: 0.8450 - val_loss: 0.7646\n",
      "Epoch 3/12\n",
      "686/686 [==============================] - 1522s 2s/step - loss: 0.5496 - val_loss: 0.7961\n",
      "Epoch 4/12\n",
      "686/686 [==============================] - 1522s 2s/step - loss: 0.4078 - val_loss: 0.8607\n",
      "Epoch 5/12\n",
      "686/686 [==============================] - 1523s 2s/step - loss: 0.3233 - val_loss: 0.9718\n",
      "Epoch 6/12\n",
      "686/686 [==============================] - 1523s 2s/step - loss: 0.2704 - val_loss: 1.0505\n",
      "Epoch 7/12\n",
      "686/686 [==============================] - 1523s 2s/step - loss: 0.2310 - val_loss: 1.2586\n",
      "Epoch 8/12\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2016Restoring model weights from the end of the best epoch.\n",
      "686/686 [==============================] - 1522s 2s/step - loss: 0.2016 - val_loss: 1.4104\n",
      "Epoch 00008: early stopping\n",
      "training model for CV #4\n",
      "Epoch 1/12\n",
      "686/686 [==============================] - 1523s 2s/step - loss: 1.3137 - val_loss: 0.9787\n",
      "Epoch 2/12\n",
      "686/686 [==============================] - 1528s 2s/step - loss: 0.7749 - val_loss: 0.7483\n",
      "Epoch 3/12\n",
      "686/686 [==============================] - 1535s 2s/step - loss: 0.5098 - val_loss: 0.8010\n",
      "Epoch 4/12\n",
      "686/686 [==============================] - 1548s 2s/step - loss: 0.3799 - val_loss: 0.8331\n",
      "Epoch 5/12\n",
      "686/686 [==============================] - 1552s 2s/step - loss: 0.3024 - val_loss: 0.9926\n",
      "Epoch 6/12\n",
      "686/686 [==============================] - 1538s 2s/step - loss: 0.2498 - val_loss: 1.0740\n",
      "Epoch 7/12\n",
      "686/686 [==============================] - 1528s 2s/step - loss: 0.2212 - val_loss: 1.1831\n",
      "Epoch 8/12\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.1912Restoring model weights from the end of the best epoch.\n",
      "686/686 [==============================] - 1538s 2s/step - loss: 0.1912 - val_loss: 1.2862\n",
      "Epoch 00008: early stopping\n",
      "training model for CV #5\n",
      "Epoch 1/12\n",
      "686/686 [==============================] - 1548s 2s/step - loss: 1.2974 - val_loss: 0.9177\n",
      "Epoch 2/12\n",
      "686/686 [==============================] - 1543s 2s/step - loss: 0.7633 - val_loss: 0.7773\n",
      "Epoch 3/12\n",
      "686/686 [==============================] - 1546s 2s/step - loss: 0.5087 - val_loss: 0.7897\n",
      "Epoch 4/12\n",
      "686/686 [==============================] - 1542s 2s/step - loss: 0.3799 - val_loss: 0.8915\n",
      "Epoch 5/12\n",
      "686/686 [==============================] - 1543s 2s/step - loss: 0.3127 - val_loss: 0.9796\n",
      "Epoch 6/12\n",
      "686/686 [==============================] - 1543s 2s/step - loss: 0.2618 - val_loss: 1.0672\n",
      "Epoch 7/12\n",
      "686/686 [==============================] - 1542s 2s/step - loss: 0.2267 - val_loss: 1.0983\n",
      "Epoch 8/12\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2024Restoring model weights from the end of the best epoch.\n",
      "686/686 [==============================] - 1543s 2s/step - loss: 0.2024 - val_loss: 1.1719\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "p_val = np.zeros((trn.shape[0], n_class))\n",
    "p_tst = np.zeros((tst.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(trn, y), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    clf = get_model()\n",
    "    \n",
    "    clf.fit(trn[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(trn[i_val], to_categorical(y[i_val])),\n",
    "            epochs=10,\n",
    "            batch_size=64,\n",
    "            )\n",
    "    p_val[i_val, :] = clf.predict(trn[i_val])\n",
    "    p_tst += clf.predict(tst) / n_fold\n",
    "    \n",
    "    del clf\n",
    "    clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:49:26.006929Z",
     "start_time": "2020-11-16T03:43:39.128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (CV):  71.7852%\n",
      "Log Loss (CV):   0.7673\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p_val, axis=1)) * 100:8.4f}%')\n",
    "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p_val):8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:38:46.904728Z",
     "start_time": "2020-11-16T03:38:46.679657Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(p_val_file, p_val, fmt='%.6f', delimiter=',')\n",
    "np.savetxt(p_tst_file, p_tst, fmt='%.6f', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:38:46.936698Z",
     "start_time": "2020-11-16T03:38:46.906758Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxUTpZnPEXJX",
    "outputId": "02c9e1f2-439d-48a3-f78e-27fb6932ef72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding (T (None, 250, 64)      1296000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block (TransformerB (None, 250, 64)      21088       token_and_position_embedding[0][0\n",
      "                                                                 transformer_block[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 64)           0           transformer_block[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 20)           1300        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20)           0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 5)            105         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,318,493\n",
      "Trainable params: 1,318,493\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(clf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:38:47.051761Z",
     "start_time": "2020-11-16T03:38:46.938510Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot_model(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제출 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:38:47.090647Z",
     "start_time": "2020-11-16T03:38:47.053249Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jXKoOWIC6g0",
    "outputId": "620417f7-087d-4f0f-e6c6-34af5484880c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19617, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4\n",
       "index               \n",
       "0      0  0  0  0  0\n",
       "1      0  0  0  0  0\n",
       "2      0  0  0  0  0\n",
       "3      0  0  0  0  0\n",
       "4      0  0  0  0  0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "print(sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:38:47.128377Z",
     "start_time": "2020-11-16T03:38:47.092090Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4S8eUx5FDFO",
    "outputId": "e31d03c3-b143-4922-ccb6-a6f9fcf02efc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.5061</td>\n",
       "      <td>0.2948</td>\n",
       "      <td>0.0161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0966</td>\n",
       "      <td>0.4303</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>0.3074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9733</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0402</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.1466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.3722</td>\n",
       "      <td>0.0912</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.4197</td>\n",
       "      <td>0.0825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4\n",
       "index                                        \n",
       "0      0.0647  0.1183  0.5061  0.2948  0.0161\n",
       "1      0.0966  0.4303  0.0641  0.1016  0.3074\n",
       "2      0.9733  0.0112  0.0039  0.0008  0.0107\n",
       "3      0.0402  0.0094  0.7650  0.0388  0.1466\n",
       "4      0.3722  0.0912  0.0344  0.4197  0.0825"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[sub.columns] = p_tst\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T03:38:47.293017Z",
     "start_time": "2020-11-16T03:38:47.130230Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "tZhbUjhXE3Yr",
    "outputId": "6642ce6e-22c5-4f87-a4f1-5bdae56abaad"
   },
   "outputs": [],
   "source": [
    "sub.to_csv(sub_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/kgsc8254/anaconda3/lib/python3.8/site-packages (0.1.91)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.94-cp38-cp38-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.91\n",
      "    Uninstalling sentencepiece-0.1.91:\n",
      "      Successfully uninstalled sentencepiece-0.1.91\n",
      "Successfully installed sentencepiece-0.1.94\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /home/kgsc8254/anaconda3/lib/python3.8/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/kgsc8254/anaconda3/lib/python3.8/site-packages (from tensorflow_hub) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/kgsc8254/anaconda3/lib/python3.8/site-packages (from tensorflow_hub) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/kgsc8254/anaconda3/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /home/kgsc8254/anaconda3/lib/python3.8/site-packages (1.0.4)\n",
      "Requirement already satisfied: six in /home/kgsc8254/anaconda3/lib/python3.8/site-packages (from bert-tensorflow) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = Dense(units)\n",
    "    self.W2 = Dense(units)\n",
    "    self.V = Dense(1)\n",
    "\n",
    "  def call(self, values, query): # 단, key와 value는 같음\n",
    "    # query shape == (batch_size, hidden size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # score 계산을 위해 뒤에서 할 덧셈을 위해서 차원을 변경해줍니다.\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = 20000\n",
    "#maxlen = 250\n",
    "maxlen = 500\n",
    "vocab_size = 20000\n",
    "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size, 128, input_length=maxlen, mask_zero = True)(inputs)\n",
    "    \n",
    "    lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences = True))(embedding_layer)\n",
    "    lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(64, dropout=0.5, |\n",
    "                                            return_sequences=True, return_state=True))(lstm)\n",
    "   \n",
    "    \n",
    "    state_h = Concatenate()([forward_h, backward_h]) # 은닉 상태\n",
    "    state_c = Concatenate()([forward_c, backward_c]) # 셀 상태\n",
    "    \n",
    "    attention = BahdanauAttention(64) # 가중치 크기 정의\n",
    "    context_vector, attention_weights = attention(lstm, state_h)\n",
    "    \n",
    "    dense1 = Dense(20, activation=\"relu\")(context_vector)\n",
    "\n",
    "    dropout = Dropout(0.5)(dense1)\n",
    "\n",
    "    outputs = Dense(n_class, activation=\"sigmoid\")(dropout)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model for CV #1\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 1253s 913ms/step - loss: 1.3603 - val_loss: 0.9928\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 1230s 896ms/step - loss: 0.9135 - val_loss: 0.7374\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 1222s 891ms/step - loss: 0.6944 - val_loss: 0.6727\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 1219s 888ms/step - loss: 0.5693 - val_loss: 0.6960\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 1224s 892ms/step - loss: 0.4886 - val_loss: 0.7483\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 1223s 891ms/step - loss: 0.4403 - val_loss: 0.7764\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 1223s 891ms/step - loss: 0.4017 - val_loss: 0.9171\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 1235s 900ms/step - loss: 0.3718 - val_loss: 0.8136\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 1242s 905ms/step - loss: 0.3373 - val_loss: 0.8941\n",
      "Epoch 10/10\n",
      "1372/1372 [==============================] - 1229s 895ms/step - loss: 0.3194 - val_loss: 0.9823\n",
      "training model for CV #2\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 1246s 908ms/step - loss: 1.2309 - val_loss: 0.8683\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 1231s 897ms/step - loss: 0.8265 - val_loss: 0.7776\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 1236s 901ms/step - loss: 0.6980 - val_loss: 0.7721\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 1237s 902ms/step - loss: 0.6292 - val_loss: 0.7892\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 1245s 907ms/step - loss: 0.5820 - val_loss: 0.8370\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 1237s 901ms/step - loss: 0.5450 - val_loss: 0.8187\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 1226s 894ms/step - loss: 0.4984 - val_loss: 0.8961\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 1229s 895ms/step - loss: 0.4598 - val_loss: 0.9569\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 1232s 898ms/step - loss: 0.4315 - val_loss: 0.9977\n",
      "Epoch 10/10\n",
      "1372/1372 [==============================] - 1217s 887ms/step - loss: 0.4063 - val_loss: 1.0419\n",
      "training model for CV #3\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 1241s 904ms/step - loss: 1.4349 - val_loss: 1.2798\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 1234s 899ms/step - loss: 1.2383 - val_loss: 1.2181\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 1232s 898ms/step - loss: 1.0608 - val_loss: 1.0498\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 1226s 893ms/step - loss: 0.9464 - val_loss: 1.0584\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 1229s 896ms/step - loss: 0.8702 - val_loss: 1.0595\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 1227s 894ms/step - loss: 0.7434 - val_loss: 0.8699\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 1225s 893ms/step - loss: 0.5980 - val_loss: 0.8688\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 1224s 892ms/step - loss: 0.5208 - val_loss: 0.8418\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 1221s 890ms/step - loss: 0.4392 - val_loss: 0.8359\n",
      "Epoch 10/10\n",
      "1372/1372 [==============================] - 1226s 894ms/step - loss: 0.3815 - val_loss: 0.8990\n",
      "training model for CV #4\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 1242s 905ms/step - loss: 1.2325 - val_loss: 0.8231\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 1247s 909ms/step - loss: 0.8038 - val_loss: 0.6874\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 1244s 907ms/step - loss: 0.6305 - val_loss: 0.6504\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 1244s 907ms/step - loss: 0.5204 - val_loss: 0.7185\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 1239s 903ms/step - loss: 0.4578 - val_loss: 0.7210\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 1244s 907ms/step - loss: 0.4030 - val_loss: 0.8099\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 1251s 912ms/step - loss: 0.3691 - val_loss: 0.8429\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 1254s 914ms/step - loss: 0.3416 - val_loss: 0.9080\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 1249s 910ms/step - loss: 0.3134 - val_loss: 0.9272\n",
      "Epoch 10/10\n",
      "1372/1372 [==============================] - 1252s 913ms/step - loss: 0.2894 - val_loss: 1.0711\n",
      "training model for CV #5\n",
      "Epoch 1/10\n",
      "1372/1372 [==============================] - 915s 667ms/step - loss: 1.3963 - val_loss: 1.0593\n",
      "Epoch 2/10\n",
      "1372/1372 [==============================] - 913s 665ms/step - loss: 0.9571 - val_loss: 0.8043\n",
      "Epoch 3/10\n",
      "1372/1372 [==============================] - 909s 663ms/step - loss: 0.7683 - val_loss: 0.7815\n",
      "Epoch 4/10\n",
      "1372/1372 [==============================] - 909s 662ms/step - loss: 0.6754 - val_loss: 0.7833\n",
      "Epoch 5/10\n",
      "1372/1372 [==============================] - 908s 662ms/step - loss: 0.6175 - val_loss: 0.7773\n",
      "Epoch 6/10\n",
      "1372/1372 [==============================] - 909s 662ms/step - loss: 0.5658 - val_loss: 0.7606\n",
      "Epoch 7/10\n",
      "1372/1372 [==============================] - 909s 662ms/step - loss: 0.4862 - val_loss: 0.7471\n",
      "Epoch 8/10\n",
      "1372/1372 [==============================] - 908s 662ms/step - loss: 0.4299 - val_loss: 0.8416\n",
      "Epoch 9/10\n",
      "1372/1372 [==============================] - 906s 660ms/step - loss: 0.3864 - val_loss: 0.8728\n",
      "Epoch 10/10\n",
      "1372/1372 [==============================] - 920s 670ms/step - loss: 0.3589 - val_loss: 0.9284\n"
     ]
    }
   ],
   "source": [
    "p_val = np.zeros((trn.shape[0], n_class))\n",
    "p_tst = np.zeros((tst.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(trn, y), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    clf = get_model()\n",
    "    \n",
    "    clf.fit(trn[i_trn], \n",
    "            to_categorical(y[i_trn]),\n",
    "            validation_data=(trn[i_val], to_categorical(y[i_val])),\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            )\n",
    "    p_val[i_val, :] = clf.predict(trn[i_val])\n",
    "    p_tst += clf.predict(tst) / n_fold\n",
    "    \n",
    "    del model\n",
    "    clear_session()\n",
    "    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (CV):  75.5116%\n",
      "Log Loss (CV):   1.0137\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p_val, axis=1)) * 100:8.4f}%')\n",
    "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p_val):8.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 250, 128)     2560000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 250, 128)     98816       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 250, 128), ( 98816       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "bahdanau_attention (BahdanauAtt ((None, 128), (None, 16577       bidirectional_1[0][0]            \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 20)           2580        bahdanau_attention[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 5)            105         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,776,894\n",
      "Trainable params: 2,776,894\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(clf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "sub[sub.columns] = p_tst\n",
    "sub.to_csv('../sub/AttentionLstmTest.csv')#0.3774 contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#stopword&alpha_num 여부 X  \n",
    " Accuracy (CV):  76.8072%  \n",
    " Log Loss (CV):   0.6412...contest   0.33039  \n",
    " \n",
    "#stopword&alpha_num 여부 O  \n",
    "Accuracy (CV):  72.8238%  \n",
    "Log Loss (CV):   0.7361  \n",
    "\n",
    "#stopword&alpha_num 여부 O + earlyStop patience=5-->7에서 멈춤 (이전엔 5였음)  \n",
    "Accuracy (CV):  72.2808%  \n",
    "Log Loss (CV):   0.7593  \n",
    "\n",
    "#stopword&alpha_num 여부 X + maxlen 700  \n",
    "Accuracy (CV):  71.7852%  \n",
    "Log Loss (CV):   0.7673.. contest:0.40904\n",
    "\n",
    "\n",
    "#stopword&alpha_num 여부 X + BahdanauAttention lstm  \n",
    "Accuracy (CV):  76.1165%  \n",
    "Log Loss (CV):   0.6881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "sample_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PythonHome_p36",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
