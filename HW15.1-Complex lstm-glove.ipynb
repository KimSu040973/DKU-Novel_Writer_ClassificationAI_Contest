{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams, pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization,Dense, SpatialDropout1D, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "rcParams['figure.figsize'] = (16, 8)\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('max_columns', 100)\n",
    "pd.set_option(\"display.precision\", 4)\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir    = Path('C:\\\\Users\\\\USER\\\\Desktop\\\\open\\\\')\n",
    "tst_dir     = Path('C:\\\\Users\\\\USER\\\\Desktop\\\\open\\\\')\n",
    "feature_dir = Path('C:\\\\Users\\\\USER\\\\Desktop\\\\open\\\\feature\\\\')\n",
    "sub_dir     = Path('C:\\\\Users\\\\USER\\\\Desktop\\\\open\\\\sub\\\\')\n",
    "val_dir     = Path('C:\\\\Users\\\\USER\\\\Desktop\\\\open\\\\val\\\\')\n",
    "\n",
    "trn_file = data_dir / 'train.csv'\n",
    "tst_file = data_dir / 'test_x.csv'\n",
    "sample_file = data_dir / 'sample_submission.csv'\n",
    "\n",
    "target_col = 'author'\n",
    "n_fold = 5\n",
    "n_class = 5\n",
    "seed = 42\n",
    "\n",
    "algo_name = 'lstm'\n",
    "feature_name = 'emb'\n",
    "model_name = f'{algo_name}_{feature_name}'\n",
    "\n",
    "feature_file = feature_dir / f'{feature_name}.csv'\n",
    "p_val_file = val_dir / f'{model_name}.val.csv'\n",
    "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
    "sub_file = sub_dir / f'{model_name}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(trn_file, index_col=0)\n",
    "test = pd.read_csv(tst_file, index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_num(text):\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stopwords:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \n",
    "             \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \n",
    "             \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \n",
    "             \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \n",
    "             \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \n",
    "             \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n",
    "             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \n",
    "             \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \n",
    "             \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \n",
    "             \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \n",
    "             \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 적용\n",
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "train['text'] = train['text'].apply(alpha_num).apply(remove_stopwords)\n",
    "test['text'] = test['text'].apply(alpha_num).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879,) (19617,) (54879,)\n"
     ]
    }
   ],
   "source": [
    "# train test 분리\n",
    "X_train = train['text'].values\n",
    "X_test = test['text'].values\n",
    "y = train['author'].values\n",
    "print(X_train.shape, X_test.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54879, 400) (19617, 400)\n"
     ]
    }
   ],
   "source": [
    "#파라미터 설정\n",
    "vocab_size = 15000\n",
    "embedding_dim = 64\n",
    "max_length = 400\n",
    "padding_type='post'\n",
    "#tokenizer에 fit\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "#데이터를 sequence로 변환해주고 padding\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "trn = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
    "tst = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n",
    "print(trn.shape, tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43903, 400) (43903, 5)\n",
      "(10976, 400) (10976, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = pd.get_dummies(train['author']).values\n",
    "#Y = dataset3['user_suggestion'].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(trn,y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_file = 'C:\\\\Users\\\\user\\\\Desktop\\\\open\\\\glove.6B.200d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(glove_file,encoding='UTF8') \n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 200\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 200)          3000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 400, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 400, 128)          135680    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 400, 128)          512       \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 400, 128)          98816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 400, 128)          512       \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 3,345,733\n",
      "Trainable params: 3,344,773\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length = max_length, weights = [embedding_matrix]))\n",
    "#model.add(Conv1D(128, 5, activation = 'relu'))\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "138/138 [==============================] - 4662s 34s/step - loss: 1.6438 - accuracy: 0.2861 - val_loss: 1.4935 - val_accuracy: 0.3478\n",
      "Epoch 2/5\n",
      "138/138 [==============================] - 5138s 37s/step - loss: 1.3668 - accuracy: 0.4296 - val_loss: 1.3010 - val_accuracy: 0.4669\n",
      "Epoch 3/5\n",
      "138/138 [==============================] - 5557s 40s/step - loss: 1.2026 - accuracy: 0.5128 - val_loss: 1.1335 - val_accuracy: 0.5510\n",
      "Epoch 4/5\n",
      "138/138 [==============================] - 5874s 43s/step - loss: 1.0850 - accuracy: 0.5705 - val_loss: 1.0739 - val_accuracy: 0.5708\n",
      "Epoch 5/5\n",
      "138/138 [==============================] - 5767s 42s/step - loss: 0.9770 - accuracy: 0.6175 - val_loss: 0.9640 - val_accuracy: 0.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x245cc42cb50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "batch_size = 256\n",
    " \n",
    "model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_tst += clf.predict(tst)\n",
    "pred = np.argmax(model.predict(X_test), axis = 1)\n",
    "actual = np.argmax(Y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LSTM  is 0.6325619533527697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(actual, pred)\n",
    "print(\"Accuracy of LSTM  is {}\".format(acc))#0.6348396501457726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19617, 5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(tst)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(sample_file, index_col=0)\n",
    "sub[sub.columns] = pred\n",
    "sub.to_csv(sub_file)#0.829608 dacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
